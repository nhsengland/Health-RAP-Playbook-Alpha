{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Health Reproducible Analytical Pipelines (RAP) Playbook","text":"<p>This website documents processes, tools and technologies used in health when creating Reproducible Analytical Pipelines (RAP)</p> <p>It is intended to complement, not replace, existing UK Government and NHS guidance:</p> <ul> <li>The cross-government big picture on RAP: Government Analysis Function RAP Strategy </li> <li>The cross-government RAP community and guidance: Government Analysis Function RAP Guidance </li> <li>Guidance on how to set up a RAP squad and train others in RAP: NHS England RAP Community of Practice</li> <li>Other RAP-related guidance:</li> <li>Quality Assurance of Code for Analysis and Research (Duck book)</li> <li>NHS Software Engineering Quality Framework</li> <li>Turing Way: A handbook to reproducible, ethical and collaborative data science</li> <li>Why open source is important:</li> <li>Cabinet office - Be open and use open source</li> <li>NHS open source digital playbook</li> </ul>"},{"location":"#alpha-please-note-that-this-website-is-currently-in-alpha-testing-as-such-you-may-encounter-bugs-errors-or-inaccuracies-during-your-visit-your-feedback-is-invaluable-in-helping-us-improve-this-site","title":"ALPHA Please note that this website is currently in alpha testing. As such, you may encounter bugs, errors, or inaccuracies during your visit. Your feedback is invaluable in helping us improve this site.","text":""},{"location":"#who-this-is-for","title":"Who this is for","text":"<ul> <li>Analysts and researchers using health data who want to adopt RAP  </li> <li>Analyst leaders in health who want to support or encourage RAP adoption  </li> <li>IT, cyber, and Information Governance professionals who want to ensure RAP is applied safely to health data  </li> <li>Members of the public who want to understand how their data is processed to provide insights into health and care</li> </ul>"},{"location":"#how-to-use-this-playbook","title":"How to use this playbook","text":"<p>In our initial release we have focussed on guidance for those who have organisational buy-in and are looking for guidance and best practice on tools and processes:</p> <ul> <li>How do I do RAP in health  \u2013 deals with the difficulties of doing RAP when you have sensitive data</li> <li>Who else is doing it  \u2013 links to testimony and resources from other health organisations who are doing RAP</li> <li>Exceptions to open sharing of RAP \u2013 this discusses situations where it may be necessary not to publish your code </li> <li>Levels of RAP maturity \u2013 this gives guidance on how to stage your RAP strategy to achieve high value gains early on in your journey</li> </ul>"},{"location":"#future-releases-will-contain","title":"Future releases will contain:","text":"<p>Guidance for those less familiar with RAP. You will probably need to convince someone in your organisation (or yourself) that it\u2019s a good thing to do:  </p> <ul> <li>Why is it important to health \u2013 this covers the wider policy ask, and points to the drivers for this work.  </li> <li>What is RAP in health \u2013 this covers why we need RAP in health analytics, key considerations, and misconceptions.  </li> </ul> <p>Guidance if you are one of the thousands of analysts working across hundreds of organisations in Trusts, PCNs, and ICBs:  </p> <ul> <li>RAP for ICBs Trusts PCNs \u2013 bespoke resources and contacts for those working in smaller local teams</li> </ul>"},{"location":"Exceptions_to_open_sharing_of_RAP/","title":"Exceptions to open sharing of RAP","text":"<p>It is much easier to write your analysis code as a RAP from the start of a project than it is to make an existing project reproducible later, because you can address issues and ensure quality as you progress. If you do not do this, you will have to spend significant amounts of time at the end of the project checking your code is of high quality, that the analysis is correct, and that colleagues can recreate the results in the event that you are unavailable or move to a different team and the analysis needs to be repeated.  Adopting the principals of RAP from the beginning means you can:</p> <ul> <li> <p>Follow best practices from the beginning, such as ensuring documentation is well kept, passwords are not hard coded into the scripts, and code is clear and well structured.</p> </li> <li> <p>Allow colleagues to identify parts of your code for reuse which you might not have recognised yourself.</p> </li> <li> <p>Enable colleagues to contribute ideas as the project is in progress, for example if colleagues know of existing algorithms and techniques that could help.</p> </li> </ul> <p>Whilst all analysis should be created using a RAP approach, an assessment will be required as to whether the RAP should be made publicly available and openly shared, or if the RAP should only be made available to a specific team or department within an organisation.</p>"},{"location":"Exceptions_to_open_sharing_of_RAP/#when-to-adopt-an-open-sharing-approach","title":"When to Adopt an Open Sharing Approach","text":"<p>The default position should be to use RAP approaches for any analysis with a view that the code, documentation, and results will be openly shared and made publicly available.  Only if an exception is identified should the RAP not be made available publicly.  Openly sharing RAPs can lead to a number of additional benefits such as:</p> <ul> <li> <p>More robust code and analysis.  As other organisations and the general public can scrutinise the code and the analysis, they may be able to identify areas that can be improved as there is an outside view that provides a different perspective or experience.  </p> </li> <li> <p>It may help other researchers. Aside from enabling other health researchers to see what methods are used, openly sharing RAP may help reduce the effort to start analysis.  For example, if a RAP has a verified and validated set of codes to create cohorts of patients with Type 2 diabetes, the code may be utilised by other research groups or organisations who are studying Type 2 diabetes.</p> </li> <li> <p>Ensure confidence that results are valid and health data is being used to benefit public health.</p> </li> <li> <p>Support any journal or conference manuscript submitted for peer review as the analytics as well as the results can be reviewed in conjunction with the manuscript, resulting in far more robust conclusions.</p> </li> </ul> <p>WWhilst RAP should be made open, there are times when it is preferable to delay this. Research groups could delay making any RAP documentation and code available until after publication of their work or at the point of publication to maintain a competitive advantage or refine code in response to reviewer feedback. RAP code and documentation should also not be released before policy decisions have publicly announced as the analytical code will be linked to sensitive decisions that may be going through reviews and iterations. Regardless of any delays, it is still good practice to ultimately make any analytical code open.</p>"},{"location":"Exceptions_to_open_sharing_of_RAP/#when-not-to-adopt-an-open-sharing-approach","title":"When Not to Adopt an Open Sharing Approach","text":"<p>Whilst it is of great benefit to ensuring open and robust analysis of healthcare data, it may not always be appropriate to openly publish RAP documentation and code.  </p> <p>For example:  </p> <ul> <li> <p>Exposing proprietary or protected intellectual property rights: Some analytics will interface with data structures or systems that are copyrighted and are the intellectual property of the organisation that created them.  If RAP artefacts that interface with items are made open, copyrighted and protected intellectual property rights will be made available to the organisation\u2019s competitors.</p> </li> <li> <p>Competitive advantage: The UK is a world leader at developing innovative medicines, devices, and digital tools which save lives. Academic, charitable, and commercial institutions spend billions of pounds annually on these innovations, which can only be made available to patients through robust and rigorous clinical research. Making RAP artefacts that contribute to this research open would prevent organisations from having a competitive advantage and hinder research.</p> </li> <li> <p>Protecting patient privacy: Some analytics on health care data may risk identifying patients, such as rare diseases that have small cell counts or analysis on data from patients in consented studies.  If there is a risk that individuals may be identified from the analysis, then the RAP code and artefacts should remain private.</p> </li> <li> <p>Data and cyber security considerations: Whilst it is good practice to ensure that passwords, private keys or any type of secret are not hard coded into source code, there are times when this is unavoidable.  For example, the way APIs are called in analytics code may be used by hackers to gain access to systems.  </p> </li> <li> <p>During policy formation, where publishing your code could prejudice future decision making.</p> </li> </ul>"},{"location":"Exceptions_to_open_sharing_of_RAP/#summary","title":"Summary","text":"<ul> <li>If you can code in the open from the get go this will save you time later on.</li> <li>There are circumstances where being too open may cause problems such as;<ul> <li>Interfering with bidding and procurement processes.</li> <li>Release of IP.</li> <li>Release of commercially sensitive info.</li> <li>Where release of partial information could create a distorted or biased view of a situation.</li> </ul> </li> </ul>"},{"location":"How_do_I_do_rap_in_health/","title":"How do I do RAP in health?","text":"<p>Note</p> <p>For more information on:</p> <ul> <li>Who else is doing RAP and why you should think about doing it, see \"Who else is doing RAP\"</li> <li>When you should not do RAP, see \"Permitted Exceptions to Open Sharing of Reproducible Analytical Pipelines\"</li> </ul>"},{"location":"How_do_I_do_rap_in_health/#executive-summary","title":"Executive Summary","text":"<ul> <li>Work out what you want to achieve from RAP - Pick your strategy and think about the benefits in the context of your organisation.</li> <li>Start small and build out - If your pipelines aren't very mature, the first steps might just be to get everything to baseline RAP, and focus on training your staff in the basics. You could also take some exemplar projects all the way to gold, to find out what's possible.</li> <li>Engage early with key stakeholders and discuss RAP frequently - Information Governance (IG) and cyber security teams may see risks in open-source tools, publishing code and sharing code between teams. Make sure you have mitigations in place. Platforms and IT need to be brought in to address any shortfalls in technology. Engage early with your data owners, platform owners, IG teams, and internal IT as these can all release blockers to the successful implementation of RAP in health.</li> <li>Set up a RAP champion squad - Even just two people will be crucial for helping others, figuring out workarounds, and promoting the benefits of RAP.</li> <li>Audit your tooling and existing pipelines - Recognise how mature the tools and pipelines are and identify what is missing, who needs help and what is blocking progress.</li> <li>When working with secure health data,such as patient level records, you will have to operate in a more secure environment - Extra stages will need to be added to ensure both you and the secure data are protected from accidentally releasing things that should not be released.</li> </ul> <p>This guide will be broken into sections following the Government Analysis Function RAP Strategy, discussing how to ensure we have:</p> <ul> <li>The right tools</li> <li>The right capability</li> <li>The right culture</li> </ul>"},{"location":"How_do_I_do_rap_in_health/#are-you-working-with-open-data-or-secure-health-data","title":"Are You Working with Open Data or Secure Health Data?","text":"<p>Note</p> <p>This guide will mostly be concerned with issues arising due to working on sensitive health data. Open data people can skip past the Secure Environments section and the more generic RAP guides will generally be usable.</p> <p>There are certain risks and conflicts when we combine the transparency goal of RAP, with the need to handle patient and public data in line with our obligations as health data analysts, which can be health (and NHS) data specific. For example, data versioning is a key desire of RAP, so that results can be reproduced at key points in time. However, with the National Data Opt-out, or even changes in legislation, patients may be removed from historical records.  We need to both be able to mechanically remove patients from datasets, and accept that it could alter the results leading to a level of irreproducibility.</p> <p>We MUST avoid patient and public data leaking out via transparency routes - such as accidentally committing data files to code version control repositories, or leaving record level (and unsuppressed) data visible in any outputs or from being accidentally put into repositories.</p> <p>Therefore, the first question about RAP and Health data is - is your data and work open, or does it need to be secure?</p> <p>If it's open, many of the challenges described elsewhere in this play book may not apply to you and the more generic RAP guidance available (much of which can be seen here) will generally be usable.</p> <p>If the data is secure, such as patient level records, then you will have to operate in a more secure environment, and extra stages need to be added to the RAP, to ensure both you and any secure health data are protected from accidentally releasing things that should not be released.</p>"},{"location":"How_do_I_do_rap_in_health/#some-points-to-consider-when-getting-started","title":"Some points to consider when getting started","text":"<p>In an ideal system</p> <ul> <li>Senior leadership will embrace open source and champion the benefits RAP can bring to your organisation.</li> <li>Platforms and tooling will allow use of open source analytical tools, and give analysts access to version control while maintaining data security.</li> <li>Analyst leaders will help analysts deal with blockers, and prioritise and protect efforts to do RAP</li> <li>RAP champions must get the local RAP community going, work across silos and help analysts get unstuck by understanding the local situation.</li> <li>Analysts will take advantage of existing RAP communities and contribute to them.</li> </ul> <p>Pick a strategy. This might be to get the basics right on a wide set of analytical pipelines, maximise automation, or provide a strong example of the benefits that can be achieved if you go all the way.</p> <p>Set up a RAP champion squad. Change needs people to champion it, to figure out a way forward and to help others. Even if this is only two people they can make a big difference.</p> <p>Audit the current situation in your organisation.  Identify how mature your existing pipelines are and what is possible within the current platforms. Measure these against levels of RAP, and the RAP strategy.</p> <p>Once you have a strategy you need to regularly engage with leadership in analytics, Information Governance, IT, and your risk owners to agree on your strategy. Agree on what can be done and where the risks lie, and mitigate the risks by following best practices and showing how others have done it.  </p> <p>Stay resilient as you will hit blockers. Blockers might be technical, procedural, or conflicting priorities. Refer back to your strategy regularly and use the organisation's leadership to drive change. You are not alone - you can also reach out to the wider RAP community for help.</p>"},{"location":"How_do_I_do_rap_in_health/#the-right-tools","title":"The Right Tools","text":"<p>In an ideal system...</p> <p>Senior leadership will:</p> <ul> <li>Understand the extent to which their platforms are RAP capable, and promote filling the gap.</li> <li>Set an expectation that RAP compliance can compliment meeting Information Governance and  security commitments.  </li> </ul> <p>Platforms will:</p> <ul> <li>Provide a space in which staff can meet every level and dimension of RAP - namely being able to access the required data, compute, external code (and packages), to be able share and publish code, and for each step to be automatable.</li> <li>Be as easy to use as possible, but also safe, with the appropriate security, controls, and assurance in place.</li> <li>Be conscious of existing and emerging standards, such as the NHS Secure Data Environments (SDE) requirements: It is not the job of analysts to lobby for platforms to meet these standards.</li> </ul> <p>Analysts and Data Engineer leaders (and RAP champions) will:</p> <ul> <li>Work with platforms, security, and Information Governance teams to develop platforms which are RAP compliant, flexible, and responsive whilst also meeting commitments to data protection and security.</li> <li>Ensure analysts and data engineers get access to the right tools, and that they use them.</li> </ul> <p>Analysts and Data engineers will:</p> <ul> <li>Use open-source tools where appropriate and publish their code in a safe way unless they are one of the few exceptions to this.</li> <li>Ensure data is versioned to allow for analyses to be reproduced, where possible, given health data may change such as due to national opt-outs.</li> </ul> <p>The Cross Government RAP strategy sets out that analysts often do not have the tools they need to do good analysis and to make it repeatable. Many of the issues faced by analysts in the wider sector are also experienced in health, such as:</p> <ul> <li>not having the right tools.</li> <li>the tools they have entrenching manual processes.</li> <li>analyst requirements not being a focus of the platform specification.</li> </ul> <p>This is especially true in the health sector, where due to the very sensitive nature of the data we use, cyber security and Information Governance compliance are used as excuses for creating nearly uncrossable silos, inefficient manual processes, and for not publishing code. Security through obscurity is considered insufficient and bad practice by Government Cyber Security policy (see security considerations when coding in the open).</p> <p>The tools dimension also encompasses the data we use. Often data is not very easy to do RAP on. The data itself is usually not version-controlled, making precisely recreating previous work not always possible. Some data returns change regularly, and use quite loose data collection tools (such as spreadsheets), requiring frequent adjustment of analytical processes. Other datasets hardly ever change, with many issues commonly known, but poorly documented for decades, meaning each set of analysts starts from scratch doing a lot of pre-processing.</p> <p>RAP helps solve these issues and makes things more efficient and transparent, but particularly in health, there are a number of hurdles, which require attention before RAP can really take off.</p> <p>What parts of RAP can be done regardless of technology?</p> <p>Many health analysts do not have the perfect tools for RAP, however this should not stop them adopting RAP principles. It is important to remember that something is often better than nothing, and that we need to focus on what the benefits are and how we can achieve them.</p> <p>For example, if your organisation does not have any open source tools, or git, getting started with RAP might sound impossible, but at least you could start documenting your existing processes, making flow-charts to describe any pain points, and documenting how long and how much resource they take to run.</p> <p>If you have access to any open source language, but cannot move some key part of the process off a closed source tool, work around it. RAPify those parts of the pipeline you can change, and can access.</p> <p>It is also wise to start by focussing on the parts causing you the most trouble. Use your open source tool to automate the manual steps, or the tests and checks that are done by eye.</p> <p>If it is difficult to share code between teams, or extract code and data from a secure environment, then share it when you can, perhaps at some key checkpoints in the work.</p>"},{"location":"How_do_I_do_rap_in_health/#secure-data-environments-or-trusted-research-environments","title":"Secure Data Environments (or Trusted Research Environments)","text":"<p>A lot of health data analytics, data engineering, science, and research occurs in secure data environments (SDE), also called trusted research environments (TRE).  The aim of SDEs is to provide a controlled space where analysts can use the data, safe from outside attack, but also that human error on their part won't necessarily cause a data breach.</p> <p>There are several guidelines that outline how an SDE should function and what features it should contain:</p> <ul> <li>Government policy guidelines on an SDE for NHS Health and Social Care Data</li> <li>Health Data Research UK Trusted Research Environments</li> <li>NHS England Transformation Directorate Rationale Behind Secure Data Environments</li> </ul> <p>The design of SDEs need to be carefully considered. The natural approach of security conscious architects is to lock everything down and make an environment where analysts working for different purposes and with different sets of data cannot share their code (easily), and where extracting code and results from the environment is difficult, sometimes to the extent that analysts refuse to use it. This is not only counter productive, harming the productivity of analysts and their potentially lifesaving work, but it is also in contradiction to one of the aims of the SDEs: to enable open, transparent, and collaborative working on the patient and public data.</p> <p>In the following sub-sections we will explore the different aspects of the SDEs which need to be considered in their design.</p>"},{"location":"How_do_I_do_rap_in_health/#transparency-vs-security","title":"Transparency vs Security","text":"<p>Given a key part of RAP is transparency and sharing, this can create a conflict. It is crucial we keep patients' and the public's data safe, however as the Government policy guidelines above sets out: \"code developed in secure data environments must be published in the open unless there is a specific rationale for not doing so\".</p> <p>There is risk whenever we move anything held within an SDE outside of the environment. This includes limiting the movement of:</p> <ul> <li>any analysis outputs such as aggregated data, graphs, charts, etc.</li> <li>dashboards and APIs.</li> <li>code, including packages and code repositories.</li> <li>other artefacts, such as table metadata, trained machine learning models, etc.</li> </ul> <p>It is also important to remember that risk cannot be eliminated - only managed and reduced - and the level of risk that is acceptable from different sources is a business question most likely unique to each organisation. There is also risk posed by poorly implemented, or overly tight security. If a platform is so locked down analysts and researchers cannot do their work, then this could ultimately harm patients and the public. Additionally, if it is too burdensome for analysts to share their work, or reuse the work of others, this also poses a risk, such as a risk of duplication of work, lowered productivity, and reputational damage.</p> <p>Below we will discuss some approaches to reduce the risk, however, it is crucial that we accept that transparency and sharing are not \"nice to have\". They are a core objective of RAP, and necessary to avoid duplicated work, inefficiency, and poor practice, whilst also raising public trust.</p>"},{"location":"How_do_I_do_rap_in_health/#air-gaps-working-in-an-sde","title":"Air Gaps / Working in an SDE","text":"<p>Air gaps are a nearly impenetrable barrier around the space in which code is run against the patient data within an SDE. It prevents data, files, and other connections (such as to the internet) from passing through, except via agreed, monitored, and controlled routes, such as a \"safe output service\". This is a crucial part of the security of the SDE, however it does make doing RAP more challenging, principally because it will be difficult to share any code developed within the SDE, either publicly published as recommended by NHS SDE policy guidelines, shared with colleagues, or uploaded to a package repositories such as CRAN, Pypi, etc.</p> <p>The air gap can lead to a need to break down analytical pipelines into stages. This can be due to a reduced user experience or lack of tools within the SDE making it harder to get needed packages or extensions for IDEs and leading to analysts wanting to do work, where possible, outside the SDE. For example, continuing to process aggregated results into final reports outside of the SDE. There are some use cases that simply need access to the internet or other systems, for example loading outputs onto an organisations website or supporting a dashboard.</p> <p>Depending on how the SDE is designed, the code version control setup might be located inside the air gap, pass through it, or be outside the SDE with some mechanism to pass code into it. In either case, the core purpose needs to be remembered: it is crucial that code can be backed up, shared and reused.  </p> <p>The result of this is that SDE air gaps need to be carefully designed with machine readable routes to get inputs and outputs into, and out of, the SDEs. Examples of this are:</p> <ul> <li>data APIs with credentials</li> <li>code version control</li> <li>publishing code to package repositories (both internal \"private\" ones and public global ones)</li> <li>downloading outputs for further analysis</li> <li>passing data into the system.</li> </ul>"},{"location":"How_do_I_do_rap_in_health/#repo-checks-secret-scanning","title":"Repo Checks / Secret Scanning","text":"<p>Due to the sensitive nature of health data, and the novelty of code version control to many of our analysts, within the health industry there is a particular risk of accidental disclosure of patient data via code version control, for example accidentally uploading code with a comment containing some patient data used in testing, to a git repo.  Current approaches taken by SDEs have involved having humans manually checking outputs to ensure they are not disclosive, however this is by definition very manual, not scalable, and simply not feasible for some outputs, such as dashboards and APIs, or very large codebases. Fortunately, there are other options which can be used alongside manual human checks:</p> <ul> <li>Simply adding <code>.gitignore</code> files to the git code repositories will make it harder for data files such as CSV, parquet, or Excel spreadsheets to be added accidentally to the repository.  Adding <code>.gitignore</code> files can form a standard part of the organisational git repository template (such as the govcookiecutter).</li> <li>Using Git hooks, such as pre-commit, to run scripts against the code and catch any issues prior the code landing in the repository.</li> <li>Using continuous integration and continuous deployment (CI/CD) tools, such as GitHub Actions, to scan the code for any issues prior to merges, releases, or further code promotion</li> </ul> <p>Git hooks and CI/CD rely on finding the right scripts or tools to run against the code to identify any issues. There are a number of scripts already available, of varying complexity, that can be used.  This includes simply scanning for \"secrets\", such as access tokens to AWS, to more complex machine learning approaches. Some examples available to analysts are:</p> <ul> <li>Gitleaks: a tool for detecting and preventing hardcoded secrets like passwords, api keys, and tokens in git repositories.</li> <li>Git Secrets: a tool that prevents you from committing secrets and credentials into git repositories.</li> <li>Amazon Macie: a tool that discovers sensitive data using machine learning and pattern matching, provides visibility into data security risks, and enables automated protection against those risks.</li> </ul>"},{"location":"How_do_I_do_rap_in_health/#platforms-architecture","title":"Platforms Architecture","text":"<p>Given all the considerations above, platform design needs to be thought through carefully so that it is both safe and secure, but also makes RAP as achievable as possible. It is important to note that SDEs fall into one of two categories:</p> <ol> <li>SDEs where users work inside the SDE, exposed to the data directly.</li> <li>SDEs where users work outside the SDE against dummy data, and submit their code which is approved and then taken inside the SDE and run against the real data, later returning the results (as in the OpenSAFELY model).</li> </ol> <p>The architecture of the two SDE approaches is different in some aspects, but they will need many of the same components to better meet the RAP requirements that have been discussed previously.  Some of the main considerations when designing an SDE are:</p> <ol> <li> <p>Having an on-premises, trusted package repository: This will allow your organisation to check and assure the packages (such as python packages from Pypi, Conda, Maven, etc.) used within the SDE once, after which they could be consumed freely by the SDE users. Users will need a straightforward method of requesting packages to be added to the repository, such as through editing some config file held in a git repository they can access. They will also need to be able to install packages into their working area, such as into their compute cluster / python virtual environment, etc. There also needs to be a way for users to be able to promote their code into a package within the on-prem trusted package repository, both from the shared and unshared areas.</p> </li> <li> <p>Work done needs to be reproducible, and this might include work done outside the environment (such as on another SDE): We want to encourage the transfer and re-use of code and methods and this will involve the creation of compute clusters, and virtual environments in a reproducible, and automatable manner. For example through using config files in the code repositories (requirements.txt for Python virtual environments, or json files for spark clusters). This might also need analysts to use virtualisation, such as using docker to recreate the operating system setup used in the original work and CI/CD tools need to be available to users, so that they can automate the running of tests.</p> </li> <li> <p>In those SDEs where users are exposed to the data directly, split users into silos based on their intended use of the data: This will make it difficult to pass anything between these silos, so there needs to be some way for users to share code between the silos. Without this users cannot reuse the code of their peers and will be forced to duplicate work. Code should be checked to ensure it is non-disclosive (such as using Git hooks, etc.) prior to being moved into this shared space and subsequent changes to this shared code can use the functionality of Git to only check the \"delta\", or any changes to the code, rather than the whole code again.  Exporting code outside the environment from the shared space should be straightforward as the code is already known to be non-disclosive but the process for users to get their code promoted into the shared area needs to be as simple as possible, preferably using the built in features of tools like GitHub (such as special branches, or releases).</p> </li> <li> <p>Data must be able to flow between the components of the SDE automatically: This should be in a machine readable way without human intervention. There should be stops in the process, but these should be related to checks being run, and human approvals processes not simply having a download button you must press to get the data out. Instead, APIs or other machine readable approaches should be used to allow pipelines to be automated and prevent needless manual work.</p> </li> <li> <p>All users (data scientists, data engineers, analysts, etc.) may need to productionise their code: This will involve code being run, and output, on a schedule, and then those outputs potentially flowing out to external users (such as in a dashboard). This cannot always rely on human output checkers (such as an hourly, or daily dashboard, showing COVID figures for each CCG in the UK), due to scale and fast turn around of outputs. There needs to be a way to sign-off a codebase, as trusted, so that it is known to produce safe outputs and for this to then be run with elevated permissions, potentially by-passing usual safe-guards for \"untrusted\" code, and outputting directly (such as in this case to a dashboard). The sign-off process will be the same as productionisation, and should allow finalised, signed-off code to be scheduled and run in a privileged space. It should then be difficult to change without further checks and sign-off (though potentially only of the changes rather than the whole codebase).</p> </li> <li> <p>Data science work such as machine learning models, needs extra thought: This may require the creation of a model repository and feature store, both within and cross cutting the working silos of the SDE. Data science work may require different compute, including GPU compute clusters. These should not be dismissed out of hand due to cost, as when used properly, a GPU cluster will do data science calculations more efficiently (in terms of time and money) than CPU only clusters.  Such an approach might also need the ability to compile and install \"code adjacent\" tools, for example when using the Python library PyStan, which under the hood is calling out to Stan.</p> </li> <li> <p>Users may require access to a pipeline tool: This can include tools such as airflow, make, or Dagster which enables steps in a process to be automated and scheduled.</p> </li> <li> <p>Users will need access to a command line: Much of the tooling discussed above require command line access, hence the users need access to one, and potentially a unix environment.</p> </li> <li> <p>Modern code development benefits greatly from using an integrated development environment (IDE): Tools such as VSCode, RStudio and PyCharm make code development faster, less manual, and can lead to higher quality code. It is crucial users can also install relevant extensions, which may need to be hosted in the internal package repository of the SDE.</p> </li> <li> <p>Changes and restrictions should be documented: Anything blocked, turned off, removed, altered, etc. from software being used within the SDE needs to be clearly explained to users through documentation about the SDE so that they can adapt their workflows or find alternative approaches to carrying out their tasks.</p> </li> <li> <p>Users will also need access to a more open environment than an SDE with many of the tools above, more open access to the internet and less controls is required for work on open data that is not patient level or sensitive data. This is because, though a lot of health data work is on sensitive patient level data, often work is also done on publicly available, or aggregated, non-disclosive data. This open environment needs to have access to an identical package repository, code version control instance, compute etc. as the SDE so that code made in secure environment can be used in the more open environment and vice-versa.</p> </li> </ol>"},{"location":"How_do_I_do_rap_in_health/#the-right-capability","title":"The Right Capability","text":"<p>In an ideal system...</p> <p>Senior leadership will:</p> <ul> <li>Understand what is needed for RAP</li> <li>Promote and enable capability development</li> <li>Become comfortable managing RAP-enabled teams</li> </ul> <p>Platforms colleagues will:</p> <ul> <li>Help create platforms that are intuitive and support the application of necessary skills and techniques.</li> </ul> <p>Analyst leaders (and RAP champions) will:</p> <ul> <li>Understand what is needed for RAP</li> <li>Champion RAP through their own work</li> <li>Promote, enable and support capability development in their teams</li> </ul> <p>Analyst will:</p> <ul> <li>Understand what is needed for RAP</li> <li>Pursue development of the skills and techniques necessary for RAP</li> <li>Apply RAP in their work</li> </ul> <p>The second goal of the cross-government RAP strategy states that analysts should have the capability to produce analysis in line with the RAP principles and are supported in doing so.</p>"},{"location":"How_do_I_do_rap_in_health/#understanding-what-is-required","title":"Understanding What is Required","text":"<p>As a necessary first step to developing the right capability, leaders (both at senior and more operational levels) and analysts should seek to understand RAP, particularly to understand what skills and techniques are needed. For leaders at all levels, this understanding will help them to understand the current capability position and also the scale of both the time and energy commitment needed for skills development and the potential benefits that that investment would bring. It will help leaders better understand how to manage analysts working under this framework and how they, as managers, can help deliver that change. An understanding of the specifics of RAP will also help during recruitment, enabling managers to prioritise the right kinds of skills and experience in their recruitment decisions.</p> <p>Analysts should of course also seek to understand what is required for their RAP journey, beginning with a skills audit. A framework should be used for this purpose, such as our Levels of RAP, as this will help analysts to assess where they currently sit on this framework and what they should focus on next in terms of developing themselves and their pipelines. As noted on the levels of RAP page, we are not suggesting that you should necessarily use our framework exactly, rather you may wish to adapt it to fit the needs and goals for your own organisation.</p> <p>Key aspects for both leaders and analysts to understand is that:</p> <ul> <li>RAP is not just for data scientists: the expectation is that anyone involved in the analysis of data should be pursuing RAP.</li> <li>RAP is not simply automation: it includes other aspects such as quality assurance, version control, documentation, dependency management and others.</li> <li>RAP is not an all-or-nothing exercise: even just applying some techniques will bring about benefit, and it will be easier to take a more iterative approach to development.</li> <li>Although RAP does require an initial investment in time and energy, that will be more than returned through the long term benefits it brings.</li> </ul> <p>Those responsible for developing and maintaining analytical platforms should support this process, understanding analysts' needs to discover what is required of platforms in order that they facilitate the application of RAP.</p>"},{"location":"How_do_I_do_rap_in_health/#links-to-learning-resources","title":"Links to Learning Resources","text":"<p>A number of learning pathways might be pursued to help develop the skills needed to apply the techniques outlined in our levels of RAP. These are listed below.</p> <p>Coding:</p> <ul> <li>R and Python courses on the ONS learning hub</li> <li>R for applied epidemiology and public health</li> </ul> <p>Version control:</p> <ul> <li>Guide to version control with Git on the NHS RAP Community of Practice</li> </ul> <p>Whilst the links above provide resources to developing the capability to do RAP within an organisation, this can only be achieved if there is support and time provided for analysts to improve their skills, which will require a culture of learning to be established by team leaders.</p>"},{"location":"How_do_I_do_rap_in_health/#the-right-culture","title":"The Right Culture","text":"<p>In an ideal system...</p> <p>Senior leadership will:</p> <ul> <li>Endorse RAP, a culture of transparency and sharing.</li> <li>Promote RAP and monitor the progress towards the RAP strategy within the organisation.</li> </ul> <p>Platforms colleagues will:</p> <ul> <li>Get behind RAP, and make it easy to do on their platforms.</li> <li>Make platforms that allow the regular exchange of code between analysts, and to the wider world.</li> <li>Embrace a culture of transparency in their own work.</li> <li>Cyber Security and Information Governance must not be used as excuses for not meeting RAP standards. \"Security through obscurity\" is not sufficient as a security strategy.  </li> </ul> <p>Analyst leaders (and RAP champions) will:</p> <ul> <li>Encourage and support learning and working according to RAP, including adopting a culture of transparency</li> <li>Think across pieces of work, and try and identify areas where similar work is taking place</li> </ul> <p>Analysts, Data Scientists, and Data Engineers will:</p> <ul> <li>Carry out analysis and data pipelines work in line with RAP by default, including sharing and promoting their work and reusing the work of others.</li> </ul> <p>At its heart, RAP is about people - not pipelines.  It is about changing how people think, such as creating reusable components, thinking holistically and not just about your silo, and thinking about colleagues and how if you do a good job and share your code, you save another team time and effort.  RAP is not all or nothing and it is not one-size-fits all.  No team will be able to deliver RAP perfectly, no project will be perfectly RAP.  There will be some projects that are more RAP compliant than others and there will be other teams that are more mature at RAP than others. RAP is a process of continuous improvement, where we provide better health outcomes to the public by being open, learning from each other as part of a community, and create the time and space needed to improve our skills and capabilities.</p>"},{"location":"How_do_I_do_rap_in_health/#create-and-support-the-time-needed-for-developing-skills","title":"Create and Support the Time Needed for Developing Skills","text":"<p>The largest barrier often faced in developing capability for RAP is that of having the time for making those developments. It is therefore essential that leaders recognise the importance of RAP and try wherever possible to create the time and space needed for analysts to develop new skills. It is important to recognise that enabling this skills development, particularly capitalising on opportunities presented by quieter periods, will allow analytical people and systems to become more efficient and robust in the long term. Particularly in the health sector, this will pave the way for a more efficient and effective response to any emerging health threats and pandemics.</p> <p>Leaders should also be confident and not unduly risk-averse in allowing new ideas to be tried out, to enable analysts to apply the skills they are developing. By having a greater understanding of RAP, leaders will be able to manage their teams' RAP journeys, ensuring development takes place in a way that minimises any potential risk of doing so. A consequence of trying new ideas is that leaders should expect projects to take longer and factor this in.  However, they can justify to senior colleagues any prolonged timelines through the benefits that applying RAP will yield. The next project will be faster, any updates will be faster, and the outputs more trustworthy.</p> <p>Analysts need to seek out and take up opportunities for development, prioritising in the first instance those skills and techniques that will bring about the greatest benefit to their analytical pipelines. Usually those techniques concern better automation, documentation, quality assurance, and version control.  However, skills need to be continuously updated and whilst leaders should provide the time and space for skill development, analysts are responsible for continuously developing their own skills and identifying their own skills gaps.  People learn in different ways so analysts need to factor in their own learning style as part of this development process.</p>"},{"location":"How_do_I_do_rap_in_health/#actively-support-and-build-a-community","title":"Actively Support and Build a Community","text":"<p>RAP can be a big change in process and so it is important to provide effective networks of support. While individual analysts can learn and apply RAP in isolation, it is usually advised that this is done as a community instead. Doing so will both provide better support for individual analysts in the development of skills but also provide better business continuity: if an individual develops at a much faster rate than their team, there is a risk that others may not be able to pick up their work in the event they are unavailable or leave the organisation.</p> <p>You might also consider having nominated RAP champions. These are named individuals who champion RAP across the organisation, supporting others, and might be connected to wider groups such as the Government Statistical Service (GSS) RAP Champion Network. RAP Champions can set up seminars, have awards, ask questions, and make themselves available to answer questions. Colleagues need to know they can go to their local RAP champions or to the wider RAP community in your organisation (or beyond) for troubleshooting and advice.  </p> <p>By actively taking part in the wider RAP community, it is a lot easier to break down silos and build cross-cutting, multi-disciplinary connections with other teams that you can learn from.  By breaking down silos, sharing work and ideas, reusing the work of others, and actively contributing to the community, this will ultimately make any RAP outputs more robust, more efficient, and will positively impact the health of the patients for whom we do analysis.</p>"},{"location":"How_do_I_do_rap_in_health/#be-open-and-transparent-by-default","title":"Be Open and Transparent by Default","text":"<p>Publishing by default and only withholding code and results by exception form a core part of point 3 of the Government's Technology Code of Practice (TCoP).  Openness is also referred to in point 9 of the Secure data environment for NHS health and social care data policy guidelines and the data saves lives NHS policy paper repeatedly refers to open source as part of the transformation the NHS must go through to build a more efficient, transparent and integrated health and social care system. Therefore there is support from the top of government for more open and transparent ways of working.  However, there is still a need for analysts, leaders, platforms colleagues, and cyber security and IG colleagues to move away from a culture of relying on \"security through obscurity\".</p> <p>Whilst it may initially be uncomfortable, working in the open will benefit all parts of the organisation.  It will allow researchers to view, reuse, and adapt existing code and enhance shared understanding of how the datasets in these environments are used. This will enable users to easily reproduce previous analysis, which will save time and improve the consistency and accuracy of analytical findings.</p> <p>From a platforms perspective, no solution, whether open or closed, is guaranteed to be free from cyber attacks.  However, platforms that are open tend to be more secure in the long term because issues are identified quicker and there is a focus on ensuring that all processes and features are tested rigorously before they are made live because they will be open and the reliance of \"security through obscurity\" is no longer available.</p> <p>By being open and transparent, you are implicitly inviting others beyond your immediate colleagues to review your code, systems, and analysis. This \"many eyes\" approach will enable a breadth of views, opinions, skills, and experiences to contribute and benefit from your code and this will ultimately lead to better outcomes for patients, the public, and the NHS.</p>"},{"location":"How_do_I_do_rap_in_health/#what-are-the-risks-of-not-doing-rap","title":"What are the risks of NOT doing RAP","text":"<p>Note</p> <ul> <li>senior leadership must understand not doing RAP will result in a business which is likely opaque, inefficient, brittle and hard to maintain.</li> <li>platforms colleagues must accept there is risk to the business for platforms not supporting RAP fully and work with the business to meet the platform recommendations made elsewhere in this document</li> <li>analyst leaders understood the risks of not doing RAP and impress these up on their colleagues, both above and below.</li> <li>analysts must embrace RAP, understanding that simply not changing has risks too.</li> </ul> <p>RAP can seem like a lot of work at first, especially to replace a bunch of processes which have more or less been working potentially for decades! However, in addition to the opportunity for benefits from RAP, there are also risks associated with not doing RAP.</p> <p>A core risk is that opaque legacy processes often have unquantified risks: staff move, source data and processes change, and if the existing processes aren't understood and documented well, then there is a chance that the outputs generated aren't being produced exactly as we think. By breaking processes apart into manageable, understood and well-tested chunks, RAP greatly reduces this risk.</p> <p>More generally, not embarking upon RAP means your analytical processes will be more brittle. This is more or less associated with using closed-source software or poorly designed Python/R programs and platforms, where you are \"locked in\" to certain ways of doing things: when that way of doing things is no longer available, then you've got a problem. Done properly, RAP not only describes the analytical/data process (using free, widely used and portable code) but also the exact libraries, compute, etc. use to run it - meaning the process should be able to run (and be reproduced) indefinitely (given supportive IT/platforms).</p> <p>Not following RAP, especially the code version control aspect, leaves you more exposed to issues such as losing the code, the code getting damaged beyond repair, data centres going down, losing access to shared drives where processes are held, servers crashing or major cyber security incidents. If the process, and everything used to run it, is described in a git repo, you can recover from any of the above - recover the code, roll back some damaging change (and identify how it happened), reclone the code to your systems in case your local shared-drives fail and re set-up your entire pipeline including python libraries, and even computer cluster specs, given entire system failure or compromise.</p> <p>Duplicate work, and not getting the full value out of your colleagues' work is a big risk of not doing RAP. Siloed, opaque processes are inevitably repeated by each team which needs to do the same task - this is wasteful of their time, but also means if one of those teams makes something really good, the rest of the business isn't benefiting from this. By promoting sharing, transparency and portability, RAP makes it easier to reap these benefits.</p>"},{"location":"Levels_of_RAP/","title":"Levels of RAP","text":"<p>For an introduction on what RAP is, and what benefits it can bring, please see our other guidance: \u201cHow do I do RAP in health?\u201d.</p> <p>The purpose of this document is to outline a number of levels to work towards when developing and improving upon a Reproducible Analytical Pipeline (RAP). It outlines in a very specific way what RAP is, and this levelled approach gives analysts clear direction for how to develop their own skills and how to make their analytical pipelines more robust. This enhanced understanding can be beneficial to all involved. </p> <p>Note that we are not saying this exact framework should be used by all organisations. It is important to think about what your unique priorities and needs are as an organisation, and adapt the framework to fit. These levels are simply provided as a starting point for you to use. </p> <p>It is important to stress that RAP does not need to be an all or nothing exercise. Even implementing just some of the principles outlined here will bring about improvements to your processes and outputs. Trying to achieve all of these standards in one go may be too daunting a task and so incremental improvements are the suggested way to go.</p> <p>It is also important to recognise that some of principles that form these standards may not always be possible. For example, where remote connections to databases are not permitted, this cannot be automated. In such cases, the aim should be to apply RAP to the other areas where principles can be applied. (Note though that it is preferred to have a direct connection to databases wherever possible, as any \u201cblack-box\u201d processes conducted in extracting and preparing data for analysis might undermine some of the benefits of RAP.)</p>"},{"location":"Levels_of_RAP/#overview","title":"Overview","text":"Things to focus on when getting started Things to focus on for more enhanced efficiency and reproducibility Things to focus on for even more enhancements around quality assurance and project management Have minimal manual steps for data extraction and analysis Meet the previous standard Meet the previous standard Use open source analytical software Have minimal manual steps for the production of outputs Have unit testing Integrate quality assurance checks throughout the analysis Use functions as reusable blocks of code Have error handling for functions Have well-commented code and documentation Adhere to a common code style Include documentation for functions Make code available to other analysts Have automated input data validation Use packaging Use version control software - Log data and analysis checks Use peer review to ensure reproducibility - Implement continuous integration - - Implement dependency management"},{"location":"Levels_of_RAP/#things-to-focus-on-when-getting-started","title":"Things to focus on when getting started","text":"<ul> <li>have minimal manual steps (for example, minimal copy-paste, point-click, drag-drop operations) for data extraction where permissions allow (for example, using SQL code), and for the analysis steps used to produce numbers, tables and charts</li> <li>use open source analytical software (preferably R or Python)</li> <li>integrate quality assurance checks throughout the analysis, automated where appropriate, supplemented with semi-automated and manual checks</li> <li>have well-commented code and documentation embedded as part of the project, rather than being saved elsewhere</li> <li>make code open and available to other analysts (including external users where appropriate)</li> <li>use version control software such as Git and GitHub to create and maintain a recorded history of the project</li> <li>use peer review to ensure that the analysis is accurate and reproducible, and that the pipeline meets the rest of this standard</li> </ul>"},{"location":"Levels_of_RAP/#further-details-on-these-points","title":"Further details on these points:","text":""},{"location":"Levels_of_RAP/#have-minimal-manual-steps-for-data-extraction-and-analysis","title":"Have minimal manual steps for data extraction and analysis","text":"<p>Minimising the number of manual steps helps reduce the risk of human error. While automation is encouraged, this does not mean the entire pipeline needs to be run in one step. For example, trying to pull and shape data and produce outputs in the same line(s) of code might make it difficult for other analysts to follow what you are doing or reuse parts of your code for similar projects. Instead, your project might be separated out into several scripts denoting different stages of the pipeline. For example, you might have one script that pulls the data from the data store and condenses it down into one or more minimum tidy datasets: defined as the minimum amount of data needed to complete the publication. Those datasets can then be passed through an RMarkdown script to produce the report (or easily picked up for use in different projects!).</p>"},{"location":"Levels_of_RAP/#use-open-source-analytical-software","title":"Use open source analytical software","text":"<p>Open source tools such as R or Python are generally preferred over proprietary tools because they are better at reading a range of different data formats, support modularised code, and can be used to automate the creation outputs in a range of different formats (for example, markdown, HTML, Word, Excel and PowerPoint files - this will become important for meeting the other principle on automated outputs). They also reduce (or eliminate) the number of times where data needs to be moved from one programme into another (for example, from SPSS into Excel into Word). Open source, such as R and Python, is also better for transparency compared to other proprietary software where licenses are required to run the code. Those licenses can also be expensive, whereas there are no costs involved for most applications of R and Python. </p> <p>See this handy guide: R for applied epidemiology and public health.</p>"},{"location":"Levels_of_RAP/#integrate-quality-assurance-checks-throughout-the-analysis","title":"Integrate quality assurance checks throughout the analysis","text":"<p>Simply being reproducible does not mean that you are doing your analysis correctly. It is important to quality assure each stage of the process, not just the end product. Quality assurance should align with the principles outlined in the Aqua Book and the Code of Practice for Statistics for those who have adopted the code. </p> <p>Checks should be built in at various points throughout the code to flag anything unexpected for further (human) exploration. These might print warning messages into the console for example, or output more detailed markdown files flagging where potential issues might exist. Note that while many quality assurance checks can be automated, some human input will always be needed, such as for final proof readings and checking that automated text still matches the figures. </p> <p>Broader checks should also take place, such as checking that the analysis and outputs meet user needs and checking that any assumptions about the model or method are likely to be true. Code reviews are also a sensible thing to do.</p> <p>To formalise your quality assurance processes and to keep an audit trail, it is a good idea to have defined roles (particularly the \"assurer\" and \"analyst\" roles), and keep detailed QA logs. </p>"},{"location":"Levels_of_RAP/#have-well-commented-code-and-documentation","title":"Have well-commented code and documentation","text":"<p>Having good quality documentation accompanying your code is important for reproducibility. The simplest form of documentation is code comments. These can be supplemented with additional documentation such as a \u201cREADME\u201d file. Standard Operating Procedures (SOPs), QA logs, assumptions logs are other good methods of documentation. </p> <p>Good documentation will help improve the reuse of code, either by other analysts using it for the first time or by the same analyst using it in the future. It should explain what the code does and how it can be used, including what options or parameters are available, examples of how to run it, and an explanation of any software or environment dependencies. It should be embedded as part of the project, rather than being saved elsewhere, so that it can be easily found. </p>"},{"location":"Levels_of_RAP/#make-code-available-to-other-analysts","title":"Make code available to other analysts","text":"<p>It is a good idea to make code available to others because this means that the pipeline can be reproduced should the usual analyst go on leave or leave the organisation. It should be stored in a place that is accessible to others (not on a local drive). Keeping code in a repository such as GitHub can also make it easier to find and provides a back up in case of accidental deletion (excluding anything sensitive such as datasets and database connection details).</p> <p>Ideally, the code should be made publicly available for transparency, but this may not always be appropriate. Please adhere to your organisational policy on this.</p>"},{"location":"Levels_of_RAP/#use-version-control-software","title":"Use version control software","text":"<p>Version control is important for documenting what changes have been made when, why, and by whom, as well as helping to ensure that people running your code know that they are using the right version. It can act as a safety net, allowing you to roll back to earlier versions of code in the event of an issue, or to review how an earlier report was produced.</p> <p>GitHub (or GitLab) can be used as a central repository to store your code and version history, allowing it to be easily accessed by others. Sharing code via GitHub can also help avoid duplication of work when other analysts are looking to perform similar tasks. GitHub also allows analysts to be open about areas they have identified for improvement via an issues log, also inviting ideas from others. Private repositories can be used for internal use only, or public repositories can be used to make your code more openly available. Please adhere to your organisational policy on open repositories if available.</p> <p>It is important not to commit anything sensitive to version control repositories, such as datasets, secret keys and credentials. Datasets should be excluded to avoid disclosing any personal information. It is recommended that keys and credentials should be saved in a separate file that can still be used within the project, but which is excluded from the repository. Pre-commit hooks, such as those provided by the cross-government \u201cgovcookiecutter\u201d tool, or \u201c.gitignore\u201d exclusions can help mitigate this risk. Remember that even when sensitive information is included deep within the version history, these can be checked out and viewed.</p> <p>While datasets are usually not committed to repositories, it is still important to ensure that some form of version control is applied to the source data, so that others know what version of the data was used for the analysis. If your project relies on snapshot CSV files, rather than connecting to a database directly, it is good idea to store those files in a read-only folder, to prevent them being modified or deleted - any changes like this could make it difficult to reproduce the same outputs in the future . For these files, it is important to ensure that they are saved prior to any data cleaning taking place, so that that data cleaning process can be documented within the production pipeline.</p> <p>Even where direct connections to databases are made, it may be a good idea to save a copy of a tidy dataset (the minimum dataset needed to produce an output, rather than the full underlying dataset) at some point in the pipeline to keep a record of the data used, and so that it can also be reused in other projects. Please adhere to proper information governance procedures when doing this, however, avoiding saving copies of anything that might be considered sensitive.</p>"},{"location":"Levels_of_RAP/#use-peer-review-to-ensure-reproducibility","title":"Use peer review to ensure reproducibility","text":"<p>The purpose of the peer review here is to ensure that the analysis is fully reproducible and that it does indeed appropriately meet the principles outlined in the framework above.</p> <p>It can, for example, involve a peer testing whether they can successfully run your code, checking that documentation is clear and complete, and checking that version control has been appropriately implemented. </p>"},{"location":"Levels_of_RAP/#things-to-focus-on-for-even-more-enhancements-around-quality-assurance-and-project-management","title":"Things to focus on for even more enhancements around quality assurance and project management","text":"<ul> <li>achieve all of the points above</li> <li>have minimal manual steps (for example, minimal copy-paste, point-click, drag-drop operations) for the production of spreadsheet workbooks and reports for publication</li> <li>use functions as reusable blocks of code</li> <li>adhere to a common best practice code style</li> <li>have automated input data validation  </li> </ul>"},{"location":"Levels_of_RAP/#further-details-on-these-points_1","title":"Further details on these points:","text":""},{"location":"Levels_of_RAP/#have-minimal-manual-steps-for-the-production-of-outputs","title":"Have minimal manual steps for the production of outputs","text":"<p>Automating the production of outputs, such as reports, spreadsheets or dashboards, further reduces the risk of human error. There are packages available to help produce reports automatically, such as RMarkdown and Python-Markdown. Spreadsheet workbooks can similarly be produced using the openxlsx package for R, for example. Producing outputs in these ways means that you know all elements of the output are in sync, as everything is produced from that single pipeline. With more manual approaches, forgetting to update the charts, for example, will mean that they are no longer in sync with what is presented in the text. A good test for how automated your pipeline is might be to think whether or not your outputs are deletable without worry. If you would not want to delete your outputs, because it would take time and energy to re-make them, then your end-to-end pipeline is not fully automated.</p>"},{"location":"Levels_of_RAP/#use-functions-as-reusable-blocks-of-code","title":"Use functions as reusable blocks of code","text":"<p>Functions are particularly useful when you are repeating the same operations at multiple points in your code. Rather than writing out the same code each time you need to carry out an operation, you can create a function and call that each time instead. This lessens the risk of error because if you need to change that operation, you only need to change the function definition, not each place in the code where it is run (some of which may otherwise be missed). Functions also lend themselves nicely to sharing snippets of functional code with others.</p>"},{"location":"Levels_of_RAP/#adhere-to-a-common-code-style","title":"Adhere to a common code style","text":"<p>Working to a common code style makes it easier for other analysts to pick up your code as they will be able to interpret and understand it more quickly. We will be looking to develop a code style guide in the future. For R users, the tidyverse style guide is the most commonly used style in the global community. \"PEP 8\" is an alternative for Python users. </p>"},{"location":"Levels_of_RAP/#have-automated-input-data-validation","title":"Have automated input data validation","text":"<p>Validating your input data is important as the quality of your outputs will necessarily depend on the quality of the inputs. Remember: \u201cgarbage in, garbage out\u201d. Validating those inputs in an automated way at the very start of the pipeline allows you to quickly identify and address any quality issues early. You might produce code to check, for example, for missing data, incorrect data types, extreme or outlier values, and whether there have been any unexpectedly large changes in the latest iteration of a longitudinal dataset. Other sense checks can also be performed. You could have your code produce a markdown document which flags any rows that may warrant further exploration - this may be particularly useful for any reports which are run on a regular basis.</p> <p>Some useful packages for input data validation include:</p> <ul> <li>assertr and validate for R</li> </ul>"},{"location":"Levels_of_RAP/#things-to-focus-on-for-even-more-enhancements-around-quality-assurance-and-project-management_1","title":"Things to focus on for even more enhancements around quality assurance and project management","text":"<ul> <li>achieve all of the points covered under the bronze and silver standards</li> <li>have unit testing for functions</li> <li>have error handling for functions</li> <li>include documentation of functions (usually included as part of a package)</li> <li>use packaging</li> <li>log data and analysis checks</li> <li>implement continuous integration</li> <li>implement dependency management</li> </ul>"},{"location":"Levels_of_RAP/#further-details-on-these-points_2","title":"Further details on these points:","text":""},{"location":"Levels_of_RAP/#have-unit-testing","title":"Have unit testing","text":"<p>Unit testing is where functions are tested with controlled inputs each time they are called, to check that expected outputs are returned. This can be a useful method to raise alarms if changes made to code elsewhere in the project, or some other environment change, adversely affects the function\u2019s operation. The principle behind unit testing is that where the functionality of the smallest units of the code can be guaranteed, it is more likely that the overall project is running as expected. They also encourage analysts to focus more on quality assurance, and what the purpose of each piece of code is. Unit tests further act as another layer of documentation: it tells users what the intention behind a certain piece of code was and what the expected outputs are.</p> <p>Some useful packages for unit testing include:</p> <ul> <li>testthat and tinytest for R</li> <li>pytest for Python</li> </ul>"},{"location":"Levels_of_RAP/#have-error-handling-for-functions","title":"Have error handling for functions","text":"<p>Error handling for functions involves stating how the function should behave when it encounters something unexpected, such as a missing or incorrectly formatted parameter passed through the function call. Common actions might include stopping the code with an error message, or printing a warning message but allowing the code to continue.</p>"},{"location":"Levels_of_RAP/#include-documentation-for-functions","title":"Include documentation for functions","text":"<p>It is sensible to provide documentation for your functions so that others know how to use them. Aspects of functions to document include its purpose, the parameters required in function calls along with any default values, and expected inputs and outputs. The preferred place to include this documentation would be within package help files and vignettes.</p>"},{"location":"Levels_of_RAP/#use-packaging","title":"Use packaging","text":"<p>A package more formally captures all the code and documentation required for a project or collection of functions in one place. It also allows for easier sharing of all the functions needed for a pipeline, or indeed the whole pipeline itself, as well as enhanced version control for the project (via package version numbers).</p>"},{"location":"Levels_of_RAP/#log-data-and-analysis-checks","title":"Log data and analysis checks","text":"<p>Producing logs of data and analysis checks can help analysts, users and quality assurers to understand whether anything is going wrong with the pipeline, and to understand how and where to respond to any issues. These logs might include, for example, the outputs of any input data validation, unit tests, other checks, messages and warnings, as well as more manual checks (for example, via QA logs). The Office for National Statistics are planning to produce a chapter on this in the Duck Book.</p>"},{"location":"Levels_of_RAP/#implement-continuous-integration","title":"Implement Continuous Integration","text":"<p>Continuous Integration (CI) describes the managed process of accepting and integrating changes made by individual analysts into a definitive main version of a project. For example, it includes the management of conflicts and the running of regular (automated) tests to ensure that no bugs have been introduced by recent changes. CI helps project leads to identify and resolve any conflicts and bugs early, and helps keep individual collaborators up to date with developments. Pull requests can be used in GitHub as part of this purpose, to require a sign-off step before any new code is accepted into the main pipeline. Branches can also be used to work on changes without the affecting the main pipeline until your are ready, for example to build in a new feature or when fixing a bug.</p>"},{"location":"Levels_of_RAP/#implement-dependency-management","title":"Implement dependency management","text":"<p>Dependency management is important as your code is ultimately dependent on the specific state of the software used at the time the analysis is run. If the environment differs for another analyst, or for the same analyst in the future (for example, following software and package updates), the outputs may differ. A simple illustration of this kind of issue is provided by the Turing Way Community: A basic division of 1 by 5 can either return 0 or 0.2, depending on which version of Python is being used (Python 2 defaults to integer division; Python 3 does not). To combat risks such as these, details of the environment can be stored, such as via the renv package for R or via a Docker container image. These tools save a record of the environment in which the analysis was originally performed, allowing others to restore that same environment.</p>"},{"location":"Who_else_is_doing_it/","title":"Who else is doing RAP","text":"<p>RAP was borne from the academic principle of reproducible research and is adapted to allow others producing insight from data to benefit from increased transparency, reuse and resilience in their analytical pipelines. RAP is being done across a range of organisation ion health and care.    </p>"},{"location":"Who_else_is_doing_it/#nhs-england-perspective","title":"NHS England perspective","text":"<p>NHS England has begun its journey to RAP for all our analytical outputs. The aim is to get all publications to 'baseline RAP' though some of our publications are more mature than this already.</p> <p>Some benefits already seen are: less reliance on proprietary software, more reuse of code and faster publication production.  </p> <p>Useful links, including our excellent community of practice pages, are here:  </p> <p>RAP community of practice website RAP community of practice repo NHS England RAP package template Other code from NHS England analytics services </p> <p>Publication RAP repos are listed here</p>"},{"location":"Who_else_is_doing_it/#department-of-health-social-care-dhsc-perspective","title":"Department of Health &amp; Social Care (DHSC) perspective","text":"<p>The size and maturity of the analysis function in DHSC has increased over the last three years, in response to the Covid-19 pandemic, and again upon the absorption of the Office for Health Improvements and Disparities (OHID) into the department. OHID brought with it a large and mature analytical operation as well as greater responsibility for published statistics.</p> <p>DHSC analysts access platforms owned by NHS and UK Health Security Agency (UKHSA) for purposes of working with agreed data under specific use cases. Following the Data Saves Lives strategy recommendation that \"DHSC develop its internal infrastructure for hosting, using and disseminating aggregate and non-personal datasets\"; DHSC is now working to develop new platforms and improve existing ones with the intention these support the adoption of RAP.</p> <p>While DHSC have mostly internal RAP products, with the creation of OHID now also have responsibility for publicly available tooling such as fingertips. Sections of the underlying data pipeline of fingertips have been published as packages, and are widely used beyond the department. DHSC continues to increase its training provision for the use and analysis of data and has recently doubled the size of its dedicated central Data Science Hub. The Hub is currently developing further RAP tooling to help develop, implement and manage RAPs across the department's analytical community.</p>"},{"location":"Who_else_is_doing_it/#forward-look","title":"Forward look","text":"<p>Currently DHSC's key activities are infrastructure development and tooling to support RAP. Concurrently with this development we will lay the groundwork for a strategic approach with pilots to understand our data flows allowing us to rationalise, standardise and catalogue the pipelines with a view to efficient implementation, cataloguing and maintenance. As these workstreams mature we will turn to realising and demonstrating benefits of RAP through implementation, engagement, training and evaluation.</p>"},{"location":"Who_else_is_doing_it/#uk-health-security-agency-ukhsa-perspective","title":"UK Health Security Agency (UKHSA) perspective","text":"<p>At UKHSA, we are committed to increasing the uptake of RAP across our analytical community and in meeting the goals of the wider government RAP strategy. </p> <p>Work is ongoing to develop and change our data and analytical platforms to make them more ready for RAP. We are also hard at work promoting a better awareness and understanding of RAP, and encouraging a greater readiness and motivation for the uptake of RAP across the agency.</p>"},{"location":"Who_else_is_doing_it/#how-to-let-others-find-your-work","title":"How to let others find your work","text":"<p>Topics are a great way to tag your work and find others for example  reproducible-analytical-pipeline </p> <ul> <li>Guidance on how to add topics to your repo</li> </ul>"}]}